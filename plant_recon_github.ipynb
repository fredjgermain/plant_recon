{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fredjgermain/plant_recon/blob/main/plant_recon_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbMGLZxZVQmW",
        "outputId": "13b09208-e689-4ad3-d680-3f06bd8fcc56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'plant_recon'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 0), reused 4 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n"
          ]
        }
      ],
      "source": [
        "# Before recloning from github use \"Disconnect and delete runtime\" \n",
        "!git clone https://github.com/fredjgermain/plant_recon.git \n",
        "\n",
        "# Refresh browser or wait a bit to see cloned folder appear in Colab \n",
        "# Once \"/content/plant_recon\" is done cloning in Colab \n",
        "%cd /content/plant_recon \n",
        "%ls \n",
        "!git status "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PLANT RECOGNITION\n",
        "*Description*: Train a DL model to recon plants, trees, fruits, pests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "\n",
        "[!] Find some test images "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PROJECT SETTING \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries \n",
        "import sys \n",
        "import os \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from matplotlib.pyplot import imread \n",
        "\n",
        "import tensorflow as tf \n",
        "import tensorflow_hub as hub \n",
        "\n",
        "from IPython.display import Image \n",
        "import datetime \n",
        "import json \n",
        "\n",
        "# # Load tensorboard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Print libraries versions \n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "print(\"Numpy version:\", np.__version__)\n",
        "print(\"Tensor flow version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "\n",
        "# Check for GPU -----------------------------------------\n",
        "print(\"GPU availability:\", \n",
        "      \"GPU is available\" if tf.config.list_physical_devices(\"GPU\") \n",
        "      else \"No GPU available\") \n",
        "\n",
        "# Projects paths and constants --------------------------\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/plant_recon\"\n",
        "MODEL_LOG_FILE = f'{PROJECT_PATH}/models/modeling_log.json'\n",
        "\n",
        "DEFAULT_IMG_SIZE = 224\n",
        "DEFAULT_BATCH_SIZE = 32\n",
        "\n",
        "SAMPLE_SUFFIX=\"model_sample\"\n",
        "FULL_SUFFIX=\"model_full\"\n",
        "\n",
        "SAMPLE_SIZE = 500\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions =============================================================\n",
        "# Create a time stamped string to now. \n",
        "def time_stamp() -> str: \n",
        "  return datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
        "\n",
        "# Func to plot image data batch \n",
        "def show_N_images(images, unique_labels, labels, N=25):\n",
        "  plt.figure( figsize=(10,10) )\n",
        "  for i in range(25): \n",
        "    ax = plt.subplot(5,5,i+1) \n",
        "    plt.imshow(images[i]) \n",
        "    plt.title( unique_labels[labels[i].argmax()] ) # datas == breeds \n",
        "    plt.axis(\"off\") \n",
        "\n",
        "\n",
        "# Array of P, images, integer\n",
        "def plot_pred(categories, predictions, labels, images, n=1):\n",
        "  P, label, image = predictions[n], labels[n], images[n]\n",
        "  pred_label = get_predicted_label(categories, P)\n",
        "  \n",
        "  # Plot image & remove ticks\n",
        "  plt.imshow(image)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  if pred_label == label:\n",
        "    color = \"green\"\n",
        "  else:\n",
        "    color = \"red\"\n",
        "  \n",
        "  plt.title(\"{} {:2.0f}% {} \".format( pred_label, np.max(P)*100, label), color=color)\n",
        "\n",
        "\n",
        "def plot_pred_conf(categories, predictions, labels, n=1): \n",
        "  Ps, label = predictions[n], labels[n]\n",
        "  pred_label = get_predicted_label(categories, Ps)\n",
        "  top_10_indexes = Ps.argsort()[-10:][::-1]\n",
        "  top_10_Ps = Ps[top_10_indexes]\n",
        "  top_10_predicted_labels = categories[top_10_indexes] # datas == breeds \n",
        "  top_plot = plt.bar(np.arange( len(top_10_predicted_labels)), top_10_Ps, color=\"grey\")\n",
        "  \n",
        "  plt.xticks( \n",
        "      np.arange(len(top_10_predicted_labels) ), \n",
        "      labels=top_10_predicted_labels, \n",
        "      rotation=\"vertical\") \n",
        "  \n",
        "  if np.isin(label, top_10_predicted_labels):\n",
        "    top_plot[np.argmax(top_10_predicted_labels == label)].set_color(\"green\")\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "# DATA BATCHING & IMG PROCESSING ==============================================\n",
        "# Create a function for preprocessing images \n",
        "\"\"\"\n",
        "Takes an images path and returns that images as a tensor. \n",
        "\"\"\"\n",
        "def process_image(image_path, image_size=DEFAULT_IMG_SIZE): \n",
        "  # Read image\n",
        "  image = tf.io.read_file(image_path) \n",
        "  # Turn jpeg into numerical Tensor with 3 color channels (Red, Green, Blue) \n",
        "  image = tf.image.decode_jpeg(image, channels=3) \n",
        "  # Normalize image, convert the color channel values from 0-255 to 0-1 values \n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  # Resize image to our desired value (IMG_SIZE, IMG_SIZE) \n",
        "  image = tf.image.resize(image, size=[image_size, image_size]) \n",
        "  return image \n",
        "\n",
        "# Create a function returning a tuple (image, label) \n",
        "def get_image_label(image_path, label):\n",
        "  \"\"\"\n",
        "  Returns a pair \"tensor and label\" from a image path and a string label. \n",
        "  \"\"\"\n",
        "  image = process_image(image_path)\n",
        "  return image, label\n",
        "\n",
        "\n",
        "# Turning data into Batches\n",
        "\"\"\"\n",
        "Purpose of batches: \n",
        "GPU has limited memory and thus can only processus a limited number of images at once. \n",
        "\"\"\"\n",
        "\n",
        "# Makes batches size 32 as default\n",
        "def create_data_batches(X, y=None, data_type=\"training\", batch_size=DEFAULT_BATCH_SIZE): \n",
        "  \"\"\" \n",
        "  Create batches of data out of image (X) and label (y) pairs. \n",
        "  Shuffles training data, \n",
        "  DOES NOT shuffle validation data. \n",
        "  \"\"\" \n",
        "  # data_type = \"test\" if test_data else data_type = \"validation\" if valid_data else data_type = \"training\" \n",
        "  print(f'Creating {data_type} data batches {batch_size}') \n",
        "\n",
        "  if data_type == \"testing\": \n",
        "    data = tf.data.Dataset.from_tensor_slices( (tf.constant(X)) ) \n",
        "    process_func = process_image \n",
        "  else: \n",
        "    data = tf.data.Dataset.from_tensor_slices( (tf.constant(X), tf.constant(y)) ) \n",
        "    process_func = get_image_label \n",
        "    if data_type == \"training\": \n",
        "      data = data.shuffle(buffer_size=len(X)) \n",
        "  return data.map(process_func).batch(batch_size) \n",
        "\n",
        "## from numpy.ma.core import append\n",
        "# Unbatch validation_batch data \n",
        "def unbatch(categories, batch_data): \n",
        "  images = [] \n",
        "  labels = [] \n",
        "  for image, label in batch_data.unbatch().as_numpy_iterator(): \n",
        "    images.append(image) \n",
        "    labels.append(categories[np.argmax(label)]) \n",
        "  return images, labels \n",
        "\n",
        "# unbatch data without labels \n",
        "def unbatch_without_labels(batch_data): \n",
        "  images = [] \n",
        "  for image in batch_data.unbatch().as_numpy_iterator(): \n",
        "    images.append(image) \n",
        "  return images \n",
        "\n",
        "\n",
        "\n",
        "# MODEL CREATION ===============================================================\n",
        "# See TensorFlow Hub ... (was in development back then, thus it might be different now) \n",
        "# See architecture \"mobilenet\" \n",
        "# src: \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\" \n",
        "def create_model(input_shape, output_shape, model_url):\n",
        "  print(\"Building model with:\", model_url) \n",
        "  # Setup model layers \n",
        "  model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(model_url), \n",
        "    tf.keras.layers.Dense(units=output_shape, activation=\"softmax\")\n",
        "  ]) \n",
        "  # Compile model \n",
        "  model.compile(\n",
        "      loss = tf.keras.losses.CategoricalCrossentropy(), \n",
        "      optimizer = tf.keras.optimizers.Adam(), \n",
        "      metrics = [\"accuracy\"]\n",
        "  )\n",
        "  # Build model\n",
        "  model.build(input_shape) \n",
        "  return model \n",
        "\n",
        "\n",
        "# Function to train a model \n",
        "# callbacks = [tensorboard, early_stopping] \n",
        "def train_model(model, training_batch, validation_batch, epochs, callbacks): \n",
        "  # create model\n",
        "  # model = create_model(input_shape, output_shape, model_url) \n",
        "  # tensorboard = create_tensorboard_callback(project_path) \n",
        "  model.fit(x=training_batch, \n",
        "            epochs = epochs, \n",
        "            validation_data = validation_batch, \n",
        "            validation_freq = 1, \n",
        "            callbacks = callbacks) \n",
        "  return model\n",
        "\n",
        "\n",
        "# Get predicted label ..........................................................\n",
        "def get_predicted_label(categories, predictions):\n",
        "  maxcol = np.argmax(predictions)\n",
        "  return categories[maxcol] # datas == breeds\n",
        "\n",
        "# Visualize prediction .........................................................\n",
        "def visualize_predictions(categories, predictions, labels, images):\n",
        "  i_m = 0\n",
        "  nr = 3\n",
        "  nc = 2\n",
        "  nimages = nr*nc\n",
        "  plt.figure(figsize=(10*nc, 5*nr))\n",
        "  for i in range(nimages):\n",
        "    plt.subplot(nr, 2*nc, 2*i+1) \n",
        "    plot_pred(categories=categories, \n",
        "              predictions=predictions, \n",
        "              labels=labels, \n",
        "              images=images, \n",
        "              n=i+i_m)\n",
        "    plt.subplot(nr, 2*nc, 2*i+2) \n",
        "    plot_pred_conf(categories=categories, \n",
        "              predictions=predictions, \n",
        "              labels=labels, \n",
        "              n=i+i_m)\n",
        "  plt.tight_layout(h_pad=1.0)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Creating callbacks \n",
        "# Tensorboad callback ..........................................................\n",
        "def create_tensorboard_callback(project_path):\n",
        "  \"\"\" \n",
        "    Functions to either \n",
        "    - Monitor models progress \n",
        "    - Save models progress \n",
        "    - Interupt models training to prevent overt fitting \n",
        "  \"\"\" \n",
        "  # Create logs \n",
        "  logdir = os.path.join( f'{project_path}/logs', time_stamp() )\n",
        "  return tf.keras.callbacks.TensorBoard(logdir)\n",
        "\n",
        "\n",
        "\n",
        "# SAVING MODEL =================================================================\n",
        "def save_model(project_path, model, suffix=None): \n",
        "  modeldir = os.path.join(f\"{project_path}/models\", time_stamp())\n",
        "  model_name = f\"-{suffix}.h5\"\n",
        "  print(f\"Saving model to: {model_name} at {modeldir} \")\n",
        "  model.save(f\"{modeldir}{model_name}\")\n",
        "  return f\"{modeldir}{model_name}\"\n",
        "\n",
        "def load_model(model_path): \n",
        "  print(f\"Loading model from: {model_path}\")\n",
        "  model = tf.keras.models.load_model( model_path, custom_objects={\"KerasLayer\":hub.KerasLayer}) \n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "# Read/Write model_log =========================================================\n",
        "def read_model_log(model_log_file):\n",
        "  return json_to_dict(model_log_file)\n",
        "\n",
        "def write_model_log(model_log_file, model_log):\n",
        "  dict_to_json(model_log_file, model_log)\n",
        "\n",
        "\n",
        "# JSON helper function \n",
        "def dict_to_json(jsonfile, dict): \n",
        "  with open(jsonfile, \"wb\") as f: \n",
        "    f.write(json.dumps(dict).encode(\"utf-8\")) \n",
        "  return jsonfile\n",
        "\n",
        "def json_to_dict(jsonfile):\n",
        "  with open(jsonfile, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DATA WRANGLING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read labels csv \n",
        "# Contains couple of image id and label. \n",
        "labels_csv = pd.read_csv(f'{PROJECT_PATH}/data/data.csv') \n",
        "\n",
        "# divides csv into its 2 columns\n",
        "filenames = [f'{PROJECT_PATH}/data/img/{filename}.jpg' for filename in labels_csv[\"id\"]] \n",
        "labels = labels_csv[\"label\"]\n",
        "\n",
        "# list unique labels (strings) \n",
        "categories = np.unique(labels) \n",
        "\n",
        "# list of filenames \n",
        "X = filenames \n",
        "# list of list of targets (image-file corresponds to labels? bool) \n",
        "y = [label == categories for label in labels] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Value count per categories, median, min, max value count\n",
        "categories_counts = labels.value_counts().sort_values()\n",
        "N_categories = len(categories)\n",
        "\n",
        "categories_counts_median = categories_counts.median() \n",
        "categories_counts_max = categories_counts.max() \n",
        "categories_counts_min = categories_counts.min() \n",
        "\n",
        "print(categories) \n",
        "print(\"Number of different categories:\", N_categories) \n",
        "print(\"Median N images per breed: \", categories_counts.median() ) \n",
        "\n",
        "print(\"Minimum N images per breed: \", categories_counts.values[0] ) \n",
        "print(\"Categories with least frequencies:\", categories_counts.index[0]) \n",
        "print(\"Maximum N images per breed: \", categories_counts.values[N_categories-1]) \n",
        "print(\"Categories with most frequencies:\", categories_counts.index[N_categories-1]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data sampling, splitting and batching training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  possible? : \n",
        "    create_batches before sampling, \n",
        "    sampling on the batch itself, \n",
        "    train sample model using a sample of batched data ? \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# SAMPLE DATASET ----------------------------------------\n",
        "# sampling\n",
        "sample_X = X[:SAMPLE_SIZE]\n",
        "sample_y = y[:SAMPLE_SIZE]\n",
        "# splitting\n",
        "sample_X_train, sample_X_val, sample_y_train, sample_y_val = train_test_split(\n",
        "  sample_X, sample_y, \n",
        "  test_size=0.2, \n",
        "  random_state=42 \n",
        ") \n",
        "# batching \n",
        "sample_training_batch = create_data_batches( \n",
        "  sample_X_train, sample_y_train) \n",
        "sample_validation_batch = create_data_batches( \n",
        "  sample_X_val, sample_y_val, data_type=\"validation\") \n",
        "\n",
        "\n",
        "# FULL DATASET --------------------------------------------------\n",
        "# splitting\n",
        "full_X_train, full_X_val, full_y_train, full_y_val = train_test_split(\n",
        "  X, y, \n",
        "  test_size=0.2, \n",
        "  random_state=42 \n",
        ")\n",
        "# batching \n",
        "full_training_batch = create_data_batches( \n",
        "  full_X_train, full_y_train) \n",
        "full_validation_batch = create_data_batches( \n",
        "  full_X_val, full_y_val, data_type=\"validation\") \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODELING CONSTANTS\n",
        "INPUT_SHAPE = [None, DEFAULT_IMG_SIZE, DEFAULT_IMG_SIZE, 3] \n",
        "OUTPUT_SHAPE = len(categories) \n",
        "MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\" \n",
        "\n",
        "# NUM_EPOCHS = 100 #@param {type:\"slider\", min:10, max:100}\n",
        "EPOCHS = 100 \n",
        "\n",
        "# Callback function\n",
        "tensorboard = create_tensorboard_callback(PROJECT_PATH)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3) \n",
        "CALLBACKS = [tensorboard, early_stopping] \n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"GPU availability:\", \n",
        "      \"GPU is available\" if tf.config.list_physical_devices(\"GPU\") \n",
        "      else \"No GPU available\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create sample model\n",
        "# sample_model = create_model(INPUT_SHAPE, OUTPUT_SHAPE, MODEL_URL) \n",
        "# sample_model.summary() \n",
        "\n",
        "# # Train sample model \n",
        "# sample_model = train_model( \n",
        "#     model=sample_model, \n",
        "#     training_batch=sample_training_batch, \n",
        "#     validation_batch=sample_validation_batch, \n",
        "#     epochs= EPOCHS, callbacks=CALLBACKS) \n",
        "\n",
        "# # Save sample model\n",
        "# sample_model_path = save_model(PROJECT_PATH, sample_model, SAMPLE_SUFFIX)\n",
        "\n",
        "# # Log sample model \n",
        "# model_log = read_model_log(MODEL_LOG_FILE)\n",
        "# model_log[SAMPLE_SUFFIX] = f\"{sample_model_path}\"\n",
        "# write_model_log(MODEL_LOG_FILE, model_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "full_model = create_model(INPUT_SHAPE, OUTPUT_SHAPE, MODEL_URL) \n",
        "full_model.summary() \n",
        "\n",
        "# Train model \n",
        "full_model = train_model( \n",
        "    model=full_model, \n",
        "    training_batch=full_training_batch, \n",
        "    validation_batch=full_validation_batch, \n",
        "    epochs= EPOCHS, callbacks=CALLBACKS) \n",
        "\n",
        "# Save model\n",
        "full_model_path = save_model(PROJECT_PATH, full_model, FULL_SUFFIX)\n",
        "\n",
        "# Log model \n",
        "model_log = read_model_log(MODEL_LOG_FILE)\n",
        "model_log[FULL_SUFFIX] = f\"{full_model_path}\"\n",
        "write_model_log(MODEL_LOG_FILE, model_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MODEL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Tensorboard logs \n",
        "## [!!] This bit will not work if browser blocks cookies. [!!] \n",
        "log_base_dir = f\"{PROJECT_PATH}/logs\" \n",
        "%tensorboard --logdir /content/drive/MyDrive/plant_recon/logs \n",
        "# --port=6006 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EVALUATE SAMPLE MODEL \n",
        "last_saved_model_path = read_model_log(MODEL_LOG_FILE)[FULL_SUFFIX]\n",
        "loaded_model = load_model(last_saved_model_path)\n",
        "# evaluation_training_batch = sample_training_batch\n",
        "evaluation_validation_batch = full_validation_batch\n",
        "\n",
        "predictions = loaded_model.predict(evaluation_validation_batch) \n",
        "\n",
        "# atrow = 42 \n",
        "# maxcol = np.argmax(predictions[atrow]) \n",
        "# print( f\"Max P at index: {np.max(predictions[atrow])}\" ) \n",
        "# print( f\"Sum of P at index {atrow}: {np.sum(predictions[atrow]) }\" ) # ~1 sum of probabilities \n",
        "# print( f\"Max index: {maxcol} \") \n",
        "# print( f\"Predicted label: {categories[maxcol]} \") \n",
        "# print(get_predicted_label(categories, predictions[81]))\n",
        "\n",
        "images, labels = unbatch(categories, evaluation_validation_batch) \n",
        "plot_pred(categories, predictions, labels, images, n=77) \n",
        "\n",
        "visualize_predictions(categories, predictions, labels, images) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MODEL TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load saved model\n",
        "last_saved_model_path = read_model_log(MODEL_LOG_FILE)[FULL_SUFFIX]\n",
        "loaded_model = load_model(last_saved_model_path)\n",
        "\n",
        "# read test data from filenames in test folder \n",
        "test_filenames = [f'{PROJECT_PATH}/data/test/{filename}' for filename in os.listdir(f'{PROJECT_PATH}/data/test') ] \n",
        "X = test_filenames\n",
        "\n",
        "test_batched = create_data_batches(X, data_type=\"testing\")\n",
        "test_unbatched = unbatch_without_labels(test_batched)\n",
        "\n",
        "predictions = loaded_model.predict(test_batched, verbose=1)\n",
        "predictions_labels = [get_predicted_label(categories, predictions[i]) for i in range(len(predictions))]\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i, image in enumerate(test_unbatched):\n",
        "  plt.subplot(1, 5, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title(predictions_labels[i])\n",
        "  plt.imshow(image)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOigI0fBdq/P5eL2T5w6KI5",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "52b9455925e7acba53a3a7414fd5b2e0ebcfd6406b66aa1109a996188146f314"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
